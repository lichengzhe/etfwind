"""ç®€åŒ–ç‰ˆ Worker - é‡‡é›†+åˆ†æï¼Œç»“æœå­˜JSONæ–‡ä»¶"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from collections import Counter
from loguru import logger

from src.config import settings
from src.collectors import NewsAggregator
from src.analyzers.realtime import analyze
from src.services.fund_service import fund_service

# è¾“å‡ºç›®å½•
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)

# å½’æ¡£ç›®å½•
ARCHIVE_DIR = DATA_DIR / "archive"
ARCHIVE_DIR.mkdir(exist_ok=True)

# ä¿¡å·å¤ç›˜æ•°æ®
REVIEW_FILE = DATA_DIR / "review.json"


def _parse_date(date_str: str) -> datetime | None:
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except Exception:
        return None


def _days_between(now: datetime, date_str: str) -> int | None:
    d = _parse_date(date_str)
    if not d:
        return None
    return (now.replace(tzinfo=None) - d).days


def _pick_trading_index(dates: list[str], entry_date: str) -> int | None:
    """é€‰æ‹©ä¸æ—©äº entry_date çš„ç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥ç´¢å¼•"""
    for i, d in enumerate(dates):
        if d >= entry_date:
            return i
    return None


def load_review_data() -> dict:
    if REVIEW_FILE.exists():
        try:
            return json.loads(REVIEW_FILE.read_text())
        except Exception:
            pass
    return {"signals": []}


def save_review_data(data: dict):
    REVIEW_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2))


async def update_review(result: dict, beijing_tz) -> dict:
    """æ›´æ–°ä¿¡å·å¤ç›˜æ•°æ®å¹¶è¿”å›æ±‡æ€»æŒ‡æ ‡"""
    data = load_review_data()
    signals: list[dict] = data.get("signals", [])

    now = datetime.now(beijing_tz)

    # æ·»åŠ ä»Šæ—¥ä¿¡å·ï¼ˆåªè®°å½•ä¹°å…¥ä¿¡å·ï¼‰
    today = now.strftime("%Y-%m-%d")
    sectors = result.get("sectors", [])
    today_entries = []
    for sector in sectors:
        etfs = sector.get("etfs") or []
        if not etfs:
            continue
        code = etfs[0].get("code")
        price = etfs[0].get("price")
        if not code or price is None:
            continue
        signal_text = sector.get("signal", "")
        if "ä¹°å…¥" not in signal_text:
            continue
        today_entries.append({
            "date": today,
            "sector": sector.get("name"),
            "type": "overall",
            "signal": signal_text,
            "etf_code": code,
            "entry_price": price,
        })

    if today_entries:
        signals.extend(today_entries)

    data["signals"] = signals
    data["updated_at"] = now.isoformat()
    save_review_data(data)

    # è®¡ç®—å¤ç›˜æŒ‡æ ‡ï¼ˆ1/3/7/20 äº¤æ˜“æ—¥ï¼‰
    horizons = [1, 3, 7, 20]
    summary = {
        "as_of": now.isoformat(),
        "horizons": {},
        "benchmark": {"name": "æ²ªæ·±300", "secid": "1.000300"},
    }

    benchmark_kline = await fund_service.get_kline_date_map(secid="1.000300")
    bench_dates = [d for d, _ in benchmark_kline]
    bench_closes = [c for _, c in benchmark_kline]

    codes = list({s.get("etf_code") for s in signals if s.get("etf_code")})
    code_to_kline: dict[str, list[tuple[str, float]]] = {}
    if codes:
        sem = asyncio.Semaphore(5)

        async def fetch_kline(c: str):
            async with sem:
                return await fund_service.get_kline_date_map(code=c)

        results = await asyncio.gather(*(fetch_kline(c) for c in codes))
        code_to_kline = dict(zip(codes, results))

    for h in horizons:
        returns = []
        excess = []
        for s in signals:
            entry_date = s.get("date", "")
            code = s.get("etf_code")
            kline = code_to_kline.get(code, [])
            if not kline:
                continue
            dates = [d for d, _ in kline]
            closes = [c for _, c in kline]
            idx = _pick_trading_index(dates, entry_date)
            if idx is None:
                continue
            exit_idx = idx + h
            if exit_idx >= len(closes):
                continue
            entry = closes[idx]
            exit_price = closes[exit_idx]
            ret = (exit_price - entry) / entry * 100
            returns.append(ret)

            if bench_dates and bench_closes:
                bidx = _pick_trading_index(bench_dates, entry_date)
                if bidx is not None and bidx + h < len(bench_closes):
                    bret = (bench_closes[bidx + h] - bench_closes[bidx]) / bench_closes[bidx] * 100
                    excess.append(ret - bret)
        if returns:
            win_rate = sum(1 for r in returns if r > 0) / len(returns) * 100
            avg_ret = sum(returns) / len(returns)
            summary["horizons"][str(h)] = {
                "count": len(returns),
                "win_rate": round(win_rate, 1),
                "avg_return": round(avg_ret, 2),
                "avg_excess": round(sum(excess) / len(excess), 2) if excess else 0,
            }
        else:
            summary["horizons"][str(h)] = {"count": 0, "win_rate": 0, "avg_return": 0, "avg_excess": 0}

    return summary


def archive_data(beijing_tz):
    """å½’æ¡£æ•°æ®ï¼šåªä¿å­˜æ¿å—è¶‹åŠ¿æŒ‡æ ‡ï¼Œç”¨äº7æ—¥è¶‹åŠ¿å±•ç¤º"""
    logger.info("=== å¼€å§‹å½’æ¡£æ•°æ® ===")
    now = datetime.now(beijing_tz)
    today = now.strftime("%Y-%m-%d")

    latest_file = DATA_DIR / "latest.json"
    if latest_file.exists():
        daily_file = ARCHIVE_DIR / f"latest_{today}.json"
        if not daily_file.exists():
            data = json.loads(latest_file.read_text())
            result = data.get("result", {})
            # å½’æ¡£ï¼šä¿å­˜è¶‹åŠ¿å’Œæ‘˜è¦æ•°æ®
            archive = {
                "date": today,
                "sectors": {
                    s["name"]: {"dir": s["direction"], "heat": s["heat"]}
                    for s in result.get("sectors", [])
                },
                "sentiment": result.get("sentiment", ""),
                "market_view": result.get("market_view", ""),
                "summary": result.get("summary", ""),
            }
            daily_file.write_text(json.dumps(archive, ensure_ascii=False, indent=2))
            logger.info(f"âœ… å½’æ¡£æˆåŠŸ: {daily_file.name}")
        else:
            logger.info(f"â­ï¸ ä»Šæ—¥å·²å½’æ¡£: {daily_file.name}")
    else:
        logger.warning("âš ï¸ latest.json ä¸å­˜åœ¨ï¼Œè·³è¿‡å½’æ¡£")

    # æ¸…ç†æ—§å½’æ¡£
    cleanup_archives(now)


def cleanup_archives(now: datetime):
    """æ¸…ç†å½’æ¡£ï¼š7å¤©å†…æ¯å¤©ä¿ç•™ï¼Œ30å¤©å†…æ¯å‘¨ä¿ç•™ï¼Œ1å¹´å†…æ¯æœˆä¿ç•™"""
    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"))
    logger.info(f"ğŸ“ å½’æ¡£ç›®å½•å…± {len(archive_files)} ä¸ªæ–‡ä»¶")

    cleaned = 0
    for f in archive_files:
        # è§£ææ—¥æœŸ
        try:
            date_str = f.stem.replace("latest_", "")
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            continue

        days_ago = (now.replace(tzinfo=None) - file_date).days

        # 7å¤©å†…ï¼šå…¨éƒ¨ä¿ç•™
        if days_ago <= 7:
            continue

        # 7-30å¤©ï¼šåªä¿ç•™å‘¨ä¸€
        if days_ago <= 30:
            if file_date.weekday() != 0:  # ä¸æ˜¯å‘¨ä¸€
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéå‘¨ä¸€ï¼‰")
            continue

        # 30å¤©-1å¹´ï¼šåªä¿ç•™æ¯æœˆ1å·
        if days_ago <= 365:
            if file_date.day != 1:  # ä¸æ˜¯1å·
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéæœˆåˆï¼‰")
            continue

        # è¶…è¿‡1å¹´ï¼šåˆ é™¤
        f.unlink()
        logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆè¶…è¿‡1å¹´ï¼‰")


def load_history(days: int = 7) -> list[dict]:
    """è¯»å–è¿‘Nå¤©çš„å†å²å½’æ¡£æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼šåªè¯»å–æ¿å—è¶‹åŠ¿ï¼‰"""
    logger.info(f"=== è¯»å–å†å²æ•°æ® (æœ€è¿‘{days}å¤©) ===")
    history = []

    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"), reverse=True)
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(archive_files)} ä¸ªå½’æ¡£æ–‡ä»¶")

    for f in archive_files[:days]:
        try:
            data = json.loads(f.read_text())
            date_str = f.stem.replace("latest_", "")

            # æ–°æ ¼å¼ï¼šç®€åŒ–å½’æ¡£
            if "sectors" in data and isinstance(data["sectors"], dict):
                history.append({
                    "date": date_str,
                    "sectors": data["sectors"],
                    "sentiment": data.get("sentiment", ""),
                    "market_view": data.get("market_view", ""),
                    "summary": data.get("summary", ""),
                })
                logger.info(f"  âœ… {date_str}: {len(data['sectors'])} ä¸ªæ¿å—")
                continue

            # å…¼å®¹æ—§æ ¼å¼ï¼ˆä» result æå–ï¼‰
            result = data.get("result", {})
            if result.get("sectors"):
                sectors = {
                    s["name"]: {"dir": s["direction"], "heat": s["heat"]}
                    for s in result.get("sectors", [])
                }
                history.append({
                    "date": date_str,
                    "sectors": sectors,
                    "sentiment": result.get("sentiment", ""),
                    "market_view": result.get("market_view", ""),
                    "summary": result.get("summary", ""),
                })
                logger.info(f"  âœ… {date_str}: {len(sectors)} ä¸ªæ¿å— (æ—§æ ¼å¼)")
            else:
                logger.info(f"  â­ï¸ {date_str}: æ— æ•°æ®")
        except Exception as e:
            logger.warning(f"  âŒ è¯»å– {f.name} å¤±è´¥: {e}")

    logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(history)} å¤©å†å²æ•°æ®")
    return history


def _describe_trend(arrows: list[str]) -> str:
    """æ ¹æ®ç®­å¤´åºåˆ—ç”Ÿæˆè¶‹åŠ¿æè¿°"""
    if not arrows:
        return ""

    if len(arrows) == 1:
        return "åˆ©å¥½" if arrows[0] == "â†‘" else "åˆ©ç©º" if arrows[0] == "â†“" else "ä¸­æ€§"

    # ç»Ÿè®¡è¿ç»­ç›¸åŒæ–¹å‘
    up_count = arrows.count("â†‘")
    down_count = arrows.count("â†“")

    # æ£€æŸ¥æœ€è¿‘è¶‹åŠ¿
    recent = arrows[-3:] if len(arrows) >= 3 else arrows
    recent_up = recent.count("â†‘")
    recent_down = recent.count("â†“")

    # ç”Ÿæˆæè¿°
    if all(a == "â†‘" for a in arrows):
        return "åˆ©å¥½"
    elif all(a == "â†“" for a in arrows):
        return "åˆ©ç©º"
    elif recent_up >= 2 and down_count > 0:
        return "è½¬å¥½"
    elif recent_down >= 2 and up_count > 0:
        return "è½¬å¼±"
    elif up_count > down_count:
        return "åå¥½"
    elif down_count > up_count:
        return "åå¼±"
    else:
        return "éœ‡è¡"


def format_history_context(history: list[dict]) -> str:
    """æ ¼å¼åŒ–å†å²æ•°æ®ä¸º AI ä¸Šä¸‹æ–‡ï¼ˆå†å²è§‚ç‚¹ + æ¿å—è¶‹åŠ¿ï¼‰"""
    if not history:
        return ""

    lines = []

    # æ·»åŠ å†å²å¸‚åœºè§‚ç‚¹å’Œæ‘˜è¦ï¼ˆæœ€è¿‘7å¤©ï¼‰
    history_items = []
    for h in history[:7]:
        date = h.get("date", "")
        view = h.get("market_view", "")
        summary = h.get("summary", "")
        if date and (view or summary):
            item = f"### {date}\n"
            if view:
                item += f"**è§‚ç‚¹**: {view}\n"
            if summary:
                item += f"**æ‘˜è¦**: {summary}\n"
            history_items.append(item)

    if history_items:
        lines.append("## è¿‘7æ—¥å¸‚åœºå›é¡¾")
        lines.extend(history_items)

    # æ”¶é›†æ‰€æœ‰å‡ºç°è¿‡çš„æ¿å—
    all_sectors = set()
    for h in history:
        all_sectors.update(h.get("sectors", {}).keys())

    if all_sectors:
        lines.append("## è¿‘7æ—¥æ¿å—è¶‹åŠ¿")

    # ä¸ºæ¯ä¸ªæ¿å—ç”Ÿæˆè¶‹åŠ¿ç®­å¤´
    for sector in sorted(all_sectors):
        arrows = []
        for h in reversed(history):  # ä»æ—§åˆ°æ–°
            s = h.get("sectors", {}).get(sector, {})
            d = s.get("dir", "")
            if d == "åˆ©å¥½":
                arrows.append("â†‘")
            elif d == "åˆ©ç©º":
                arrows.append("â†“")
            elif d:
                arrows.append("â†’")

        if arrows:
            arrow_str = "".join(arrows)
            # ç”Ÿæˆè¶‹åŠ¿æè¿°
            desc = _describe_trend(arrows)
            lines.append(f"- {sector}: {arrow_str} ({desc})")

    return "\n".join(lines)


def build_sector_trends(history: list[dict], current_sectors: list[dict]) -> dict:
    """æ„å»ºæ¿å—7æ—¥è¶‹åŠ¿æ•°æ®ï¼Œä¾›å‰ç«¯å±•ç¤º

    è¿”å›: {"é»„é‡‘": {"arrows": "â†‘â†‘â†‘â†‘â†‘â†‘â†‘", "desc": "7è¿åˆ©å¥½"}, ...}
    """
    trends = {}

    # å½“å‰æ¿å—ååˆ—è¡¨
    current_names = {s["name"] for s in current_sectors}

    for sector_name in current_names:
        arrows = []
        # ä»å†å²æ•°æ®ä¸­æå–ï¼ˆä»æ—§åˆ°æ–°ï¼‰ï¼Œæ²¡æåˆ°çš„å¤©æ˜¾ç¤ºä¸­æ€§
        for h in reversed(history):
            s = h.get("sectors", {}).get(sector_name, {})
            d = s.get("dir", "")
            if d == "åˆ©å¥½":
                arrows.append("â†‘")
            elif d == "åˆ©ç©º":
                arrows.append("â†“")
            else:
                arrows.append("â†’")  # æ²¡æåˆ°æˆ–ä¸­æ€§éƒ½æ˜¾ç¤ºâ†’

        # æ·»åŠ ä»Šæ—¥
        current = next((s for s in current_sectors if s["name"] == sector_name), None)
        if current:
            d = current.get("direction", "")
            if d == "åˆ©å¥½":
                arrows.append("â†‘")
            elif d == "åˆ©ç©º":
                arrows.append("â†“")
            else:
                arrows.append("â†’")

        if arrows:
            trends[sector_name] = {
                "arrows": "".join(arrows[-7:]),  # æœ€å¤š7å¤©
                "desc": _describe_trend(arrows[-7:])
            }

    return trends


async def save_news(news_items, beijing_tz):
    """ä¿å­˜æ–°é—»åˆ—è¡¨"""
    aggregator_urls = [
        "https://www.jin10.com/",
        "https://wallstreetcn.com/live",
        "https://kuaixun.eastmoney.com/",
    ]
    news_list = [
        {
            "title": item.title,
            "source": item.source,
            "url": item.url,
            "published_at": item.published_at.isoformat() if item.published_at else None,
        }
        for item in news_items
        if item.url and not any(item.url.startswith(agg) for agg in aggregator_urls)
    ]
    news_file = DATA_DIR / "news.json"
    news_file.write_text(json.dumps({
        "news": news_list,
        "updated_at": datetime.now(beijing_tz).isoformat(),
    }, ensure_ascii=False, indent=2))
    logger.info(f"æ–°é—»åˆ—è¡¨å·²ä¿å­˜åˆ° {news_file}")


async def run():
    """è¿è¡Œé‡‡é›†å’Œåˆ†æ"""
    logger.info("=" * 50)
    logger.info("ğŸš€ ETFé£å‘æ ‡ - å¼€å§‹è¿è¡Œ")
    logger.info("=" * 50)

    # é‡‡é›†
    logger.info("=== ç¬¬1æ­¥: é‡‡é›†æ–°é—» ===")
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        source_stats = dict(Counter(item.source for item in news.items))
        logger.info(f"âœ… é‡‡é›†å®Œæˆ: {len(news.items)} æ¡æ–°é—»")
        for src, cnt in sorted(source_stats.items(), key=lambda x: -x[1]):
            logger.info(f"  - {src}: {cnt} æ¡")
    finally:
        await agg.close()

    # æ–°é—»æ•°é‡æ£€æŸ¥
    MIN_NEWS_COUNT = 20
    if len(news.items) < MIN_NEWS_COUNT:
        logger.warning(f"âš ï¸ æ–°é—»æ•°é‡ä¸è¶³ ({len(news.items)} < {MIN_NEWS_COUNT})ï¼Œè·³è¿‡åˆ†æ")
        return None

    # è¯»å– sector_listï¼ˆä» etf_master.jsonï¼‰
    logger.info("=== ç¬¬2æ­¥: è¯»å–æ¿å—é…ç½® ===")
    sector_list = None
    master_file = Path(__file__).parent.parent / "config" / "etf_master.json"
    if master_file.exists():
        try:
            master_data = json.loads(master_file.read_text())
            sector_list = master_data.get("sector_list", [])
            logger.info(f"âœ… è¯»å–åˆ° {len(sector_list)} ä¸ªå¯é€‰æ¿å—")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å– etf_master.json å¤±è´¥: {e}")
    else:
        logger.warning("âš ï¸ etf_master.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¿å—")

    # è¯»å–å†å²æ•°æ®ç”¨äºç»¼åˆåˆ†æ
    history = load_history(days=7)
    history_context = format_history_context(history)
    if history_context:
        logger.info(f"ğŸ“œ å†å²ä¸Šä¸‹æ–‡:\n{history_context}")

    # AI åˆ†æ
    logger.info("=== ç¬¬3æ­¥: AI åˆ†æ ===")
    result = await analyze(news.items, sector_list=sector_list, history_context=history_context)

    # æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦æœ‰æ•ˆ
    output_file = DATA_DIR / "latest.json"
    beijing_tz = timezone(timedelta(hours=8))

    # å…ˆå½’æ¡£å½“å‰æ•°æ®
    archive_data(beijing_tz)

    # AI åˆ†æç»“æœæ— æ•ˆæ—¶ï¼Œä¸è¦†ç›–æ–‡ä»¶
    if not result or not result.get("sectors"):
        logger.error("âŒ AI åˆ†æç»“æœä¸ºç©ºï¼Œä¸è¦†ç›–å†å²æ•°æ®")
        if output_file.exists():
            try:
                old_data = json.loads(output_file.read_text())
                result = old_data.get("result", {})
                logger.info("ğŸ“‚ ä½¿ç”¨å†å²åˆ†æç»“æœ")
            except Exception as e:
                logger.error(f"âŒ è¯»å–å†å²æ•°æ®å¤±è´¥: {e}")
        await save_news(news.items, beijing_tz)
        logger.info("âš ï¸ è¿è¡Œç»“æŸï¼ˆåˆ†æå¤±è´¥ï¼‰")
        return None

    # åˆ†ææˆåŠŸ
    sectors = result.get("sectors", [])
    logger.info(f"âœ… AI åˆ†æå®Œæˆ: {len(sectors)} ä¸ªæ¿å—")
    for s in sectors:
        logger.info(f"  - {s['name']}: {s['direction']} {'â˜…'*s['heat']}")

    # ä¸ºæ¯ä¸ªæ¿å—åŒ¹é… ETF
    logger.info("=== ç¬¬4æ­¥: åŒ¹é… ETF ===")
    await enrich_sectors_with_etfs(result)

    # ä¿å­˜ç»“æœ
    logger.info("=== ç¬¬5æ­¥: ä¿å­˜ç»“æœ ===")
    output = {
        "result": result,
        "updated_at": datetime.now(beijing_tz).isoformat(),
        "news_count": len(news.items),
        "source_stats": source_stats,
    }

    output_file.write_text(json.dumps(output, ensure_ascii=False, indent=2))
    logger.info(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {output_file}")

    # ä¿å­˜æ–°é—»åˆ—è¡¨
    await save_news(news.items, beijing_tz)

    logger.info("=" * 50)
    logger.info("ğŸ‰ ETFé£å‘æ ‡ - è¿è¡Œå®Œæˆ")
    logger.info("=" * 50)

    return output


async def ai_map_to_master_sectors(
    ai_sectors: list[str], master_sectors: list[str]
) -> dict[str, list[str]]:
    """AI å°†åˆ†æå‡ºçš„æ¿å—æ˜ å°„åˆ° master ä¸­çš„æ ‡å‡†æ¿å—ï¼ˆå¯ä¸€å¯¹å¤šï¼‰"""
    from src.services.ai_client import AIClient, AIRequest, parse_json_with_repair

    prompt = f"""å°†å·¦è¾¹çš„æ¿å—åæ˜ å°„åˆ°å³è¾¹æœ€ç›¸å…³çš„æ ‡å‡†æ¿å—ã€‚

## å¾…æ˜ å°„æ¿å—
{', '.join(ai_sectors)}

## æ ‡å‡†æ¿å—åˆ—è¡¨
{', '.join(master_sectors)}

## è¾“å‡ºJSON
```json
{{
  "å¾…æ˜ å°„æ¿å—": ["æ ‡å‡†æ¿å—1", "æ ‡å‡†æ¿å—2"],
  ...
}}
```

è¦æ±‚ï¼š
- æ¯ä¸ªæ¿å—å¯æ˜ å°„1-3ä¸ªç›¸å…³æ ‡å‡†æ¿å—
- å¦‚"æ–°èƒ½æºè½¦"å¯æ˜ å°„åˆ°["é”‚ç”µæ± ", "æ±½è½¦"]
- å¦‚"ç§‘æŠ€"å¯æ˜ å°„åˆ°["èŠ¯ç‰‡", "è½¯ä»¶", "AI"]
- æ— æ³•æ˜ å°„åˆ™è¿”å›ç©ºæ•°ç»„[]"""

    try:
        client = AIClient()
        text = await client.send(AIRequest(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            timeout=60,
        ))
        return parse_json_with_repair(text)
    except Exception as e:
        logger.warning(f"AIæ¿å—æ˜ å°„å¤±è´¥: {e}")
        return {}


async def enrich_sectors_with_etfs(result: dict):
    """ä¸ºæ¯ä¸ªæ¿å—åŒ¹é…ETFï¼ˆAIæ˜ å°„æ¿å— + æŒ‰æˆäº¤é‡å–Top3ï¼‰"""
    sectors = result.get("sectors", [])
    if not sectors:
        logger.warning("âš ï¸ æ— æ¿å—æ•°æ®ï¼Œè·³è¿‡ETFåŒ¹é…")
        return

    # è¯»å– ETF ä¸»æ•°æ®
    master_file = Path(__file__).parent.parent / "config" / "etf_master.json"
    if not master_file.exists():
        logger.warning("âš ï¸ etf_master.json ä¸å­˜åœ¨")
        return
    etf_master = json.loads(master_file.read_text())
    master_sectors = etf_master.get("sector_list", [])
    sector_index = etf_master.get("sectors", {})
    etfs_data = etf_master.get("etfs", {})
    logger.info(f"ğŸ“Š ETFä¸»æ•°æ®: {len(etfs_data)} ä¸ªETF, {len(master_sectors)} ä¸ªæ¿å—")

    # AI å°†åˆ†ææ¿å—æ˜ å°„åˆ° master æ ‡å‡†æ¿å—
    ai_sector_names = [s["name"] for s in sectors]
    logger.info(f"ğŸ¤– AI æ˜ å°„æ¿å—: {ai_sector_names}")
    sector_mapping = await ai_map_to_master_sectors(ai_sector_names, master_sectors)

    if not sector_mapping:
        logger.warning("âš ï¸ AIæ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŒ¹é…")
        sector_mapping = {name: [name] if name in sector_index else [] for name in ai_sector_names}

    # æ ¹æ®æ˜ å°„æ”¶é›† ETF ä»£ç ï¼ˆåˆå¹¶å¤šä¸ªæ¿å—ï¼‰
    sector_etf_codes: dict[str, list[str]] = {}
    for ai_name, master_names in sector_mapping.items():
        codes = []
        for m_name in master_names:
            if m_name in sector_index:
                codes.extend(sector_index[m_name])
        sector_etf_codes[ai_name] = codes
        if master_names:
            logger.info(f"  {ai_name} â†’ {master_names}")

    # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ ETF ä»£ç 
    codes_to_fetch = set()
    for codes in sector_etf_codes.values():
        codes_to_fetch.update(codes)

    if not codes_to_fetch:
        logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…åˆ°ETFä»£ç ")
        return

    # æ‰¹é‡è·å–ETFå®æ—¶æ•°æ®
    logger.info(f"ğŸ“ˆ è·å– {len(codes_to_fetch)} ä¸ªETFå®æ—¶æ•°æ®")
    fund_data = await fund_service.batch_get_funds(list(codes_to_fetch))

    # ä¸ºæ¯ä¸ªæ¿å—æ·»åŠ ETFä¿¡æ¯
    matched = 0
    for sector in sectors:
        sector_name = sector.get("name", "")
        codes = sector_etf_codes.get(sector_name, [])
        etfs = []
        for code in codes:
            if code in fund_data:
                etfs.append(fund_data[code])
        etfs.sort(key=lambda x: x.get("amount_yi", 0), reverse=True)
        sector["etfs"] = etfs[:3]
        if etfs:
            matched += 1
            logger.info(f"  âœ… {sector_name}: {', '.join(e['name'] for e in etfs[:3])}")

    logger.info(f"âœ… ETFåŒ¹é…å®Œæˆ: {matched}/{len(sectors)} ä¸ªæ¿å—")


if __name__ == "__main__":
    asyncio.run(run())
