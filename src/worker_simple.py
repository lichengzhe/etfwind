"""ç®€åŒ–ç‰ˆ Worker - é‡‡é›†+åˆ†æï¼Œç»“æœå­˜JSONæ–‡ä»¶"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from collections import Counter
from loguru import logger

from src.config import settings
from src.collectors import NewsAggregator
from src.analyzers.realtime import analyze
from src.services.fund_service import fund_service

# è¾“å‡ºç›®å½•
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)

# å½’æ¡£ç›®å½•
ARCHIVE_DIR = DATA_DIR / "archive"
ARCHIVE_DIR.mkdir(exist_ok=True)


def archive_data(beijing_tz):
    """å½’æ¡£æ•°æ®ï¼šå½“å¤©ä¿ç•™ï¼Œ7å¤©æ¯å¤©ä¸€ä»½ï¼Œ1æœˆæ¯å‘¨ä¸€ä»½ï¼Œ1å¹´æ¯æœˆä¸€ä»½"""
    logger.info("=== å¼€å§‹å½’æ¡£æ•°æ® ===")
    now = datetime.now(beijing_tz)
    today = now.strftime("%Y-%m-%d")

    # å½’æ¡£ latest.json åˆ°å½“å¤©
    latest_file = DATA_DIR / "latest.json"
    if latest_file.exists():
        daily_file = ARCHIVE_DIR / f"latest_{today}.json"
        if not daily_file.exists():
            import shutil
            shutil.copy(latest_file, daily_file)
            logger.info(f"âœ… å½’æ¡£æˆåŠŸ: {daily_file.name}")
        else:
            logger.info(f"â­ï¸ ä»Šæ—¥å·²å½’æ¡£: {daily_file.name}")
    else:
        logger.warning("âš ï¸ latest.json ä¸å­˜åœ¨ï¼Œè·³è¿‡å½’æ¡£")

    # æ¸…ç†æ—§å½’æ¡£
    cleanup_archives(now)


def cleanup_archives(now: datetime):
    """æ¸…ç†å½’æ¡£ï¼š7å¤©å†…æ¯å¤©ä¿ç•™ï¼Œ30å¤©å†…æ¯å‘¨ä¿ç•™ï¼Œ1å¹´å†…æ¯æœˆä¿ç•™"""
    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"))
    logger.info(f"ğŸ“ å½’æ¡£ç›®å½•å…± {len(archive_files)} ä¸ªæ–‡ä»¶")

    cleaned = 0
    for f in archive_files:
        # è§£ææ—¥æœŸ
        try:
            date_str = f.stem.replace("latest_", "")
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            continue

        days_ago = (now - file_date).days

        # 7å¤©å†…ï¼šå…¨éƒ¨ä¿ç•™
        if days_ago <= 7:
            continue

        # 7-30å¤©ï¼šåªä¿ç•™å‘¨ä¸€
        if days_ago <= 30:
            if file_date.weekday() != 0:  # ä¸æ˜¯å‘¨ä¸€
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéå‘¨ä¸€ï¼‰")
            continue

        # 30å¤©-1å¹´ï¼šåªä¿ç•™æ¯æœˆ1å·
        if days_ago <= 365:
            if file_date.day != 1:  # ä¸æ˜¯1å·
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéæœˆåˆï¼‰")
            continue

        # è¶…è¿‡1å¹´ï¼šåˆ é™¤
        f.unlink()
        logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆè¶…è¿‡1å¹´ï¼‰")


def load_history(days: int = 7) -> list[dict]:
    """è¯»å–è¿‘Nå¤©çš„å†å²å½’æ¡£æ•°æ®"""
    logger.info(f"=== è¯»å–å†å²æ•°æ® (æœ€è¿‘{days}å¤©) ===")
    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    history = []

    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"), reverse=True)
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(archive_files)} ä¸ªå½’æ¡£æ–‡ä»¶")
    for f in archive_files[:days]:
        try:
            data = json.loads(f.read_text())
            date_str = f.stem.replace("latest_", "")
            result = data.get("result", {})
            if result.get("sectors"):
                history.append({
                    "date": date_str,
                    "market_view": result.get("market_view", ""),
                    "sectors": [
                        {"name": s["name"], "heat": s["heat"], "direction": s["direction"]}
                        for s in result.get("sectors", [])
                    ]
                })
                logger.info(f"  âœ… {date_str}: {len(result['sectors'])} ä¸ªæ¿å—")
            else:
                logger.info(f"  â­ï¸ {date_str}: æ— æ¿å—æ•°æ®")
        except Exception as e:
            logger.warning(f"  âŒ è¯»å– {f.name} å¤±è´¥: {e}")

    logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(history)} å¤©å†å²æ•°æ®")
    return history


def format_history_context(history: list[dict]) -> str:
    """æ ¼å¼åŒ–å†å²æ•°æ®ä¸º AI ä¸Šä¸‹æ–‡"""
    if not history:
        return ""

    lines = ["## è¿‘æœŸå†å²åˆ†æï¼ˆä¾›å‚è€ƒï¼‰"]
    for h in history[:5]:  # æœ€å¤š5å¤©
        sectors_str = ", ".join([
            f"{s['name']}({'â†‘' if s['direction']=='åˆ©å¥½' else 'â†“' if s['direction']=='åˆ©ç©º' else '-'}{'â˜…'*s['heat']})"
            for s in h["sectors"][:4]
        ])
        lines.append(f"- {h['date']}: {h['market_view']} | {sectors_str}")

    return "\n".join(lines)


async def save_news(news_items, beijing_tz):
    """ä¿å­˜æ–°é—»åˆ—è¡¨"""
    aggregator_urls = [
        "https://www.jin10.com/",
        "https://wallstreetcn.com/live",
        "https://kuaixun.eastmoney.com/",
    ]
    news_list = [
        {
            "title": item.title,
            "source": item.source,
            "url": item.url,
            "published_at": item.published_at.isoformat() if item.published_at else None,
        }
        for item in news_items
        if item.url and not any(item.url.startswith(agg) for agg in aggregator_urls)
    ]
    news_file = DATA_DIR / "news.json"
    news_file.write_text(json.dumps({
        "news": news_list,
        "updated_at": datetime.now(beijing_tz).isoformat(),
    }, ensure_ascii=False, indent=2))
    logger.info(f"æ–°é—»åˆ—è¡¨å·²ä¿å­˜åˆ° {news_file}")


async def run():
    """è¿è¡Œé‡‡é›†å’Œåˆ†æ"""
    logger.info("=" * 50)
    logger.info("ğŸš€ ETFé£å‘æ ‡ - å¼€å§‹è¿è¡Œ")
    logger.info("=" * 50)

    # é‡‡é›†
    logger.info("=== ç¬¬1æ­¥: é‡‡é›†æ–°é—» ===")
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        source_stats = dict(Counter(item.source for item in news.items))
        logger.info(f"âœ… é‡‡é›†å®Œæˆ: {len(news.items)} æ¡æ–°é—»")
        for src, cnt in sorted(source_stats.items(), key=lambda x: -x[1]):
            logger.info(f"  - {src}: {cnt} æ¡")
    finally:
        await agg.close()

    # è¯»å– sector_listï¼ˆä» etf_master.jsonï¼‰
    logger.info("=== ç¬¬2æ­¥: è¯»å–æ¿å—é…ç½® ===")
    sector_list = None
    etf_file = DATA_DIR / "etf_master.json"
    if etf_file.exists():
        try:
            etf_data = json.loads(etf_file.read_text())
            sector_list = etf_data.get("sector_list", [])
            logger.info(f"âœ… è¯»å–åˆ° {len(sector_list)} ä¸ªå¯é€‰æ¿å—")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å– sector_list å¤±è´¥: {e}")
    else:
        logger.warning("âš ï¸ etf_master.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¿å—")

    # è¯»å–å†å²æ•°æ®ç”¨äºç»¼åˆåˆ†æ
    history = load_history(days=7)
    history_context = format_history_context(history)

    # AI åˆ†æ
    logger.info("=== ç¬¬3æ­¥: AI åˆ†æ ===")
    result = await analyze(news.items, sector_list=sector_list, history_context=history_context)

    # æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦æœ‰æ•ˆ
    output_file = DATA_DIR / "latest.json"
    beijing_tz = timezone(timedelta(hours=8))

    # å…ˆå½’æ¡£å½“å‰æ•°æ®
    archive_data(beijing_tz)

    # AI åˆ†æç»“æœæ— æ•ˆæ—¶ï¼Œä¸è¦†ç›–æ–‡ä»¶
    if not result or not result.get("sectors"):
        logger.error("âŒ AI åˆ†æç»“æœä¸ºç©ºï¼Œä¸è¦†ç›–å†å²æ•°æ®")
        if output_file.exists():
            try:
                old_data = json.loads(output_file.read_text())
                result = old_data.get("result", {})
                logger.info("ğŸ“‚ ä½¿ç”¨å†å²åˆ†æç»“æœ")
            except Exception as e:
                logger.error(f"âŒ è¯»å–å†å²æ•°æ®å¤±è´¥: {e}")
        await save_news(news.items, beijing_tz)
        await fetch_etf_map()
        logger.info("âš ï¸ è¿è¡Œç»“æŸï¼ˆåˆ†æå¤±è´¥ï¼‰")
        return None

    # åˆ†ææˆåŠŸ
    sectors = result.get("sectors", [])
    logger.info(f"âœ… AI åˆ†æå®Œæˆ: {len(sectors)} ä¸ªæ¿å—")
    for s in sectors:
        logger.info(f"  - {s['name']}: {s['direction']} {'â˜…'*s['heat']}")

    # ä¸ºæ¯ä¸ªæ¿å—åŒ¹é… ETF
    logger.info("=== ç¬¬4æ­¥: åŒ¹é… ETF ===")
    await enrich_sectors_with_etfs(result)

    # ä¿å­˜ç»“æœ
    logger.info("=== ç¬¬5æ­¥: ä¿å­˜ç»“æœ ===")
    output = {
        "result": result,
        "updated_at": datetime.now(beijing_tz).isoformat(),
        "news_count": len(news.items),
        "source_stats": source_stats,
    }

    output_file.write_text(json.dumps(output, ensure_ascii=False, indent=2))
    logger.info(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {output_file}")

    # ä¿å­˜æ–°é—»åˆ—è¡¨
    await save_news(news.items, beijing_tz)

    # ç”Ÿæˆ ETF æ¿å—æ˜ å°„ï¼ˆæ¯å¤©ä¸€æ¬¡ï¼‰
    await fetch_etf_map()

    logger.info("=" * 50)
    logger.info("ğŸ‰ ETFé£å‘æ ‡ - è¿è¡Œå®Œæˆ")
    logger.info("=" * 50)

    return output


async def enrich_sectors_with_etfs(result: dict):
    """ä¸ºæ¯ä¸ªæ¿å—åŒ¹é…äº¤æ˜“é‡æœ€å¤§çš„3ä¸ªETF"""
    sectors = result.get("sectors", [])
    if not sectors:
        logger.warning("âš ï¸ æ— æ¿å—æ•°æ®ï¼Œè·³è¿‡ETFåŒ¹é…")
        return

    # è·å–æ¿å—->ETFæ˜ å°„
    sector_map = await fund_service.get_sector_etf_map()
    if not sector_map:
        logger.warning("âš ï¸ æ— æ³•è·å–æ¿å—æ˜ å°„")
        return
    logger.info(f"ğŸ“Š æ¿å—æ˜ å°„: {len(sector_map)} ä¸ªæ¿å—")

    # æ¿å—åæ˜ å°„ï¼ˆAIè¾“å‡º -> ETFæ¿å—ï¼‰
    sector_alias = {
        "æ–°èƒ½æºè½¦": "é”‚ç”µæ± ", "æ–°èƒ½æº": "å…‰ä¼", "åˆ›æ–°è¯": "åŒ»è¯",
        "è´µé‡‘å±": "é»„é‡‘", "åˆ¸å•†": "è¯åˆ¸",
        "èŠ¯ç‰‡/åŠå¯¼ä½“": "èŠ¯ç‰‡", "åŠå¯¼ä½“": "èŠ¯ç‰‡",
    }

    # æ”¶é›†éœ€è¦æŸ¥è¯¢çš„ETFä»£ç 
    codes_to_fetch = set()
    sector_etf_mapping = {}

    for sector in sectors:
        sector_name = sector.get("name", "")
        # å…ˆå°è¯•åˆ«åæ˜ å°„
        lookup_name = sector_alias.get(sector_name, sector_name)
        for key, etfs in sector_map.items():
            if key in lookup_name or lookup_name in key:
                codes = [code for code, name in etfs[:3]]
                sector_etf_mapping[sector_name] = codes
                codes_to_fetch.update(codes)
                break

    if not codes_to_fetch:
        logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…åˆ°ETFä»£ç ")
        return

    # æ‰¹é‡è·å–ETFå®æ—¶æ•°æ®
    logger.info(f"ğŸ“ˆ è·å– {len(codes_to_fetch)} ä¸ªETFå®æ—¶æ•°æ®")
    fund_data = await fund_service.batch_get_funds(list(codes_to_fetch))

    # ä¸ºæ¯ä¸ªæ¿å—æ·»åŠ ETFä¿¡æ¯
    matched = 0
    for sector in sectors:
        sector_name = sector.get("name", "")
        codes = sector_etf_mapping.get(sector_name, [])
        etfs = []
        for code in codes:
            if code in fund_data:
                etfs.append(fund_data[code])
        etfs.sort(key=lambda x: x.get("amount_yi", 0), reverse=True)
        sector["etfs"] = etfs[:3]
        if etfs:
            matched += 1
            logger.info(f"  âœ… {sector_name}: {', '.join(e['name'] for e in etfs[:3])}")

    logger.info(f"âœ… ETFåŒ¹é…å®Œæˆ: {matched}/{len(sectors)} ä¸ªæ¿å—")


async def fetch_etf_map(force: bool = False):
    """ç”Ÿæˆ ETF Master æ•°æ®æ–‡ä»¶ï¼ˆæ¯å‘¨ä¸€æ›´æ–°ï¼Œæˆ–å¼ºåˆ¶æ›´æ–°ï¼‰"""
    etf_file = DATA_DIR / "etf_master.json"
    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ¯å‘¨ä¸€ï¼Œæˆ–å¼ºåˆ¶æ›´æ–°ï¼‰
    if not force and etf_file.exists():
        try:
            data = json.loads(etf_file.read_text())
            last_update = data.get("updated_at", "")[:10]
            last_date = datetime.fromisoformat(last_update)
            # å¦‚æœä¸Šæ¬¡æ›´æ–°åœ¨æœ¬å‘¨ä¸€ä¹‹åï¼Œè·³è¿‡
            days_since_monday = now.weekday()
            this_monday = (now - timedelta(days=days_since_monday)).date()
            if last_date.date() >= this_monday:
                logger.info(f"ETF Master æœ¬å‘¨å·²æ›´æ–°ï¼ˆ{last_update}ï¼‰ï¼Œè·³è¿‡")
                return
        except Exception:
            pass

    logger.info("ç”Ÿæˆ ETF Master æ•°æ®...")
    try:
        fund_service._etf_cache_time = 0
        master = await fund_service.build_etf_master(min_amount_yi=5.0)

        if not master.get("etfs"):
            logger.warning("æœªè·å–åˆ°ETFæ•°æ®")
            return

        output = {
            **master,
            "updated_at": now.isoformat(),
        }
        etf_file.write_text(json.dumps(output, ensure_ascii=False, indent=2))
        logger.info(f"ETF Master å·²ä¿å­˜ï¼Œå…± {len(master['etfs'])} ä¸ªETF")
    except Exception as e:
        logger.warning(f"ç”Ÿæˆ ETF Master å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(run())
