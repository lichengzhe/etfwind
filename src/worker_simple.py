"""ç®€åŒ–ç‰ˆ Worker - é‡‡é›†+åˆ†æï¼Œç»“æœå­˜JSONæ–‡ä»¶"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from collections import Counter
from loguru import logger

from src.config import settings
from src.collectors import NewsAggregator
from src.analyzers.realtime import analyze
from src.services.fund_service import fund_service

# è¾“å‡ºç›®å½•
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)

# å½’æ¡£ç›®å½•
ARCHIVE_DIR = DATA_DIR / "archive"
ARCHIVE_DIR.mkdir(exist_ok=True)


def archive_data(beijing_tz):
    """å½’æ¡£æ•°æ®ï¼šå½“å¤©ä¿ç•™ï¼Œ7å¤©æ¯å¤©ä¸€ä»½ï¼Œ1æœˆæ¯å‘¨ä¸€ä»½ï¼Œ1å¹´æ¯æœˆä¸€ä»½"""
    logger.info("=== å¼€å§‹å½’æ¡£æ•°æ® ===")
    now = datetime.now(beijing_tz)
    today = now.strftime("%Y-%m-%d")

    # å½’æ¡£ latest.json åˆ°å½“å¤©ï¼ˆå«ç²¾ç®€æ‘˜è¦ï¼‰
    latest_file = DATA_DIR / "latest.json"
    if latest_file.exists():
        daily_file = ARCHIVE_DIR / f"latest_{today}.json"
        if not daily_file.exists():
            # è¯»å–å¹¶æ·»åŠ æ‘˜è¦
            data = json.loads(latest_file.read_text())
            result = data.get("result", {})
            # FOTH Matrix å½’æ¡£
            data["foth"] = {
                "facts": result.get("facts", [])[:5],
                "opinions": result.get("opinions", {}),
                "sectors": [
                    {"name": s["name"], "heat": s["heat"], "direction": s["direction"]}
                    for s in result.get("sectors", [])[:4]
                ],
                "market_view": result.get("market_view", ""),
                "commodity_cycle": result.get("commodity_cycle", {}),
            }
            daily_file.write_text(json.dumps(data, ensure_ascii=False, indent=2))
            logger.info(f"âœ… å½’æ¡£æˆåŠŸ: {daily_file.name}")
        else:
            logger.info(f"â­ï¸ ä»Šæ—¥å·²å½’æ¡£: {daily_file.name}")
    else:
        logger.warning("âš ï¸ latest.json ä¸å­˜åœ¨ï¼Œè·³è¿‡å½’æ¡£")

    # æ¸…ç†æ—§å½’æ¡£
    cleanup_archives(now)


def cleanup_archives(now: datetime):
    """æ¸…ç†å½’æ¡£ï¼š7å¤©å†…æ¯å¤©ä¿ç•™ï¼Œ30å¤©å†…æ¯å‘¨ä¿ç•™ï¼Œ1å¹´å†…æ¯æœˆä¿ç•™"""
    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"))
    logger.info(f"ğŸ“ å½’æ¡£ç›®å½•å…± {len(archive_files)} ä¸ªæ–‡ä»¶")

    cleaned = 0
    for f in archive_files:
        # è§£ææ—¥æœŸ
        try:
            date_str = f.stem.replace("latest_", "")
            file_date = datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            continue

        days_ago = (now.replace(tzinfo=None) - file_date).days

        # 7å¤©å†…ï¼šå…¨éƒ¨ä¿ç•™
        if days_ago <= 7:
            continue

        # 7-30å¤©ï¼šåªä¿ç•™å‘¨ä¸€
        if days_ago <= 30:
            if file_date.weekday() != 0:  # ä¸æ˜¯å‘¨ä¸€
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéå‘¨ä¸€ï¼‰")
            continue

        # 30å¤©-1å¹´ï¼šåªä¿ç•™æ¯æœˆ1å·
        if days_ago <= 365:
            if file_date.day != 1:  # ä¸æ˜¯1å·
                f.unlink()
                logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆéæœˆåˆï¼‰")
            continue

        # è¶…è¿‡1å¹´ï¼šåˆ é™¤
        f.unlink()
        logger.info(f"æ¸…ç†å½’æ¡£ {f.name}ï¼ˆè¶…è¿‡1å¹´ï¼‰")


def load_history(days: int = 7) -> list[dict]:
    """è¯»å–è¿‘Nå¤©çš„å†å²å½’æ¡£æ•°æ®ï¼ˆFOTH Matrixï¼‰"""
    logger.info(f"=== è¯»å–å†å²æ•°æ® (æœ€è¿‘{days}å¤©) ===")
    history = []

    archive_files = sorted(ARCHIVE_DIR.glob("latest_*.json"), reverse=True)
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(archive_files)} ä¸ªå½’æ¡£æ–‡ä»¶")

    for f in archive_files[:days]:
        try:
            data = json.loads(f.read_text())
            date_str = f.stem.replace("latest_", "")
            result = data.get("result", {})

            # æ–°æ ¼å¼ï¼šFOTH
            foth = data.get("foth", {})
            if foth:
                history.append({"date": date_str, **foth})
                facts_count = len(foth.get("facts", []))
                logger.info(f"  âœ… {date_str}: {facts_count} facts (foth)")
                continue

            # å…¼å®¹æ—§æ ¼å¼
            if result.get("sectors"):
                history.append({
                    "date": date_str,
                    "facts": result.get("facts", result.get("key_events", [])),
                    "opinions": result.get("opinions", {}),
                    "sectors": [
                        {"name": s["name"], "heat": s["heat"], "direction": s["direction"]}
                        for s in result.get("sectors", [])[:4]
                    ],
                    "market_view": result.get("market_view", ""),
                })
                logger.info(f"  âœ… {date_str}: ä» result æå–")
            else:
                logger.info(f"  â­ï¸ {date_str}: æ— æ•°æ®")
        except Exception as e:
            logger.warning(f"  âŒ è¯»å– {f.name} å¤±è´¥: {e}")

    logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(history)} å¤©å†å²æ•°æ®")
    return history


def format_history_context(history: list[dict]) -> str:
    """æ ¼å¼åŒ–å†å²æ•°æ®ä¸º AI ä¸Šä¸‹æ–‡ï¼ˆFOTH Matrixï¼‰

    åˆ†ç¦»å±•ç¤º Facts å’Œ Opinionsï¼Œè®© AI ç‹¬ç«‹åˆ¤æ–­
    """
    if not history:
        return ""

    lines = ["## å†å²æ•°æ®ï¼ˆFOTH Matrixï¼‰"]

    # History Facts
    lines.append("\n### History Factsï¼ˆå®¢è§‚äº‹ä»¶ï¼‰")
    for h in history[:3]:
        facts = h.get("facts", [])
        if facts:
            lines.append(f"**{h['date']}**: {'; '.join(facts[:3])}")

    # History Opinions
    lines.append("\n### History Opinionsï¼ˆå¸‚åœºæƒ…ç»ªï¼‰")
    for h in history[:3]:
        opinions = h.get("opinions", {})
        sectors = h.get("sectors", [])
        if opinions or sectors:
            sentiment = opinions.get("sentiment", "")
            hot_words = opinions.get("hot_words", [])
            sector_str = ", ".join(
                f"{s['name']}{'â†‘' if s['direction']=='åˆ©å¥½' else 'â†“'}"
                for s in sectors[:3]
            )
            parts = []
            if sentiment:
                parts.append(sentiment)
            if hot_words:
                parts.append(f"çƒ­è¯:{','.join(hot_words[:3])}")
            if sector_str:
                parts.append(f"çƒ­ç‚¹:{sector_str}")
            if parts:
                lines.append(f"**{h['date']}**: {' | '.join(parts)}")

    # History Commodity Cycleï¼ˆå•†å“å‘¨æœŸï¼‰
    lines.append("\n### History Commodity Cycleï¼ˆå•†å“å‘¨æœŸï¼‰")
    for h in history[:3]:
        cycle = h.get("commodity_cycle", {})
        if cycle:
            stage_name = cycle.get("stage_name", "")
            if stage_name:
                lines.append(f"**{h['date']}**: {stage_name}")

    return "\n".join(lines)


async def save_news(news_items, beijing_tz):
    """ä¿å­˜æ–°é—»åˆ—è¡¨"""
    aggregator_urls = [
        "https://www.jin10.com/",
        "https://wallstreetcn.com/live",
        "https://kuaixun.eastmoney.com/",
    ]
    news_list = [
        {
            "title": item.title,
            "source": item.source,
            "url": item.url,
            "published_at": item.published_at.isoformat() if item.published_at else None,
        }
        for item in news_items
        if item.url and not any(item.url.startswith(agg) for agg in aggregator_urls)
    ]
    news_file = DATA_DIR / "news.json"
    news_file.write_text(json.dumps({
        "news": news_list,
        "updated_at": datetime.now(beijing_tz).isoformat(),
    }, ensure_ascii=False, indent=2))
    logger.info(f"æ–°é—»åˆ—è¡¨å·²ä¿å­˜åˆ° {news_file}")


async def run():
    """è¿è¡Œé‡‡é›†å’Œåˆ†æ"""
    logger.info("=" * 50)
    logger.info("ğŸš€ ETFé£å‘æ ‡ - å¼€å§‹è¿è¡Œ")
    logger.info("=" * 50)

    # é‡‡é›†
    logger.info("=== ç¬¬1æ­¥: é‡‡é›†æ–°é—» ===")
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        source_stats = dict(Counter(item.source for item in news.items))
        logger.info(f"âœ… é‡‡é›†å®Œæˆ: {len(news.items)} æ¡æ–°é—»")
        for src, cnt in sorted(source_stats.items(), key=lambda x: -x[1]):
            logger.info(f"  - {src}: {cnt} æ¡")
    finally:
        await agg.close()

    # æ–°é—»æ•°é‡æ£€æŸ¥
    MIN_NEWS_COUNT = 20
    if len(news.items) < MIN_NEWS_COUNT:
        logger.warning(f"âš ï¸ æ–°é—»æ•°é‡ä¸è¶³ ({len(news.items)} < {MIN_NEWS_COUNT})ï¼Œè·³è¿‡åˆ†æ")
        return None

    # è¯»å– sector_listï¼ˆä» etf_master.jsonï¼‰
    logger.info("=== ç¬¬2æ­¥: è¯»å–æ¿å—é…ç½® ===")
    sector_list = None
    master_file = Path(__file__).parent.parent / "config" / "etf_master.json"
    if master_file.exists():
        try:
            master_data = json.loads(master_file.read_text())
            sector_list = master_data.get("sector_list", [])
            logger.info(f"âœ… è¯»å–åˆ° {len(sector_list)} ä¸ªå¯é€‰æ¿å—")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å– etf_master.json å¤±è´¥: {e}")
    else:
        logger.warning("âš ï¸ etf_master.json ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¿å—")

    # è¯»å–å†å²æ•°æ®ç”¨äºç»¼åˆåˆ†æ
    history = load_history(days=7)
    history_context = format_history_context(history)
    if history_context:
        logger.info(f"ğŸ“œ å†å²ä¸Šä¸‹æ–‡:\n{history_context}")

    # AI åˆ†æ
    logger.info("=== ç¬¬3æ­¥: AI åˆ†æ ===")
    result = await analyze(news.items, sector_list=sector_list, history_context=history_context)

    # æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦æœ‰æ•ˆ
    output_file = DATA_DIR / "latest.json"
    beijing_tz = timezone(timedelta(hours=8))

    # å…ˆå½’æ¡£å½“å‰æ•°æ®
    archive_data(beijing_tz)

    # AI åˆ†æç»“æœæ— æ•ˆæ—¶ï¼Œä¸è¦†ç›–æ–‡ä»¶
    if not result or not result.get("sectors"):
        logger.error("âŒ AI åˆ†æç»“æœä¸ºç©ºï¼Œä¸è¦†ç›–å†å²æ•°æ®")
        if output_file.exists():
            try:
                old_data = json.loads(output_file.read_text())
                result = old_data.get("result", {})
                logger.info("ğŸ“‚ ä½¿ç”¨å†å²åˆ†æç»“æœ")
            except Exception as e:
                logger.error(f"âŒ è¯»å–å†å²æ•°æ®å¤±è´¥: {e}")
        await save_news(news.items, beijing_tz)
        logger.info("âš ï¸ è¿è¡Œç»“æŸï¼ˆåˆ†æå¤±è´¥ï¼‰")
        return None

    # åˆ†ææˆåŠŸ
    sectors = result.get("sectors", [])
    logger.info(f"âœ… AI åˆ†æå®Œæˆ: {len(sectors)} ä¸ªæ¿å—")
    for s in sectors:
        logger.info(f"  - {s['name']}: {s['direction']} {'â˜…'*s['heat']}")

    # ä¸ºæ¯ä¸ªæ¿å—åŒ¹é… ETF
    logger.info("=== ç¬¬4æ­¥: åŒ¹é… ETF ===")
    await enrich_sectors_with_etfs(result)

    # ä¿å­˜ç»“æœ
    logger.info("=== ç¬¬5æ­¥: ä¿å­˜ç»“æœ ===")
    output = {
        "result": result,
        "updated_at": datetime.now(beijing_tz).isoformat(),
        "news_count": len(news.items),
        "source_stats": source_stats,
    }

    output_file.write_text(json.dumps(output, ensure_ascii=False, indent=2))
    logger.info(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {output_file}")

    # ä¿å­˜æ–°é—»åˆ—è¡¨
    await save_news(news.items, beijing_tz)

    logger.info("=" * 50)
    logger.info("ğŸ‰ ETFé£å‘æ ‡ - è¿è¡Œå®Œæˆ")
    logger.info("=" * 50)

    return output


async def ai_map_to_master_sectors(
    ai_sectors: list[str], master_sectors: list[str]
) -> dict[str, list[str]]:
    """AI å°†åˆ†æå‡ºçš„æ¿å—æ˜ å°„åˆ° master ä¸­çš„æ ‡å‡†æ¿å—ï¼ˆå¯ä¸€å¯¹å¤šï¼‰"""
    import httpx
    from src.config import settings

    prompt = f"""å°†å·¦è¾¹çš„æ¿å—åæ˜ å°„åˆ°å³è¾¹æœ€ç›¸å…³çš„æ ‡å‡†æ¿å—ã€‚

## å¾…æ˜ å°„æ¿å—
{', '.join(ai_sectors)}

## æ ‡å‡†æ¿å—åˆ—è¡¨
{', '.join(master_sectors)}

## è¾“å‡ºJSON
```json
{{
  "å¾…æ˜ å°„æ¿å—": ["æ ‡å‡†æ¿å—1", "æ ‡å‡†æ¿å—2"],
  ...
}}
```

è¦æ±‚ï¼š
- æ¯ä¸ªæ¿å—å¯æ˜ å°„1-3ä¸ªç›¸å…³æ ‡å‡†æ¿å—
- å¦‚"æ–°èƒ½æºè½¦"å¯æ˜ å°„åˆ°["é”‚ç”µæ± ", "æ±½è½¦"]
- å¦‚"ç§‘æŠ€"å¯æ˜ å°„åˆ°["èŠ¯ç‰‡", "è½¯ä»¶", "äººå·¥æ™ºèƒ½"]
- æ— æ³•æ˜ å°„åˆ™è¿”å›ç©ºæ•°ç»„[]"""

    try:
        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(
                f"{settings.claude_base_url.rstrip('/')}/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": settings.claude_api_key,
                    "anthropic-version": "2023-06-01",
                },
                json={
                    "model": settings.claude_model,
                    "max_tokens": 1024,
                    "messages": [{"role": "user", "content": prompt}]
                },
            )
            resp.raise_for_status()
            text = resp.json()["content"][0]["text"].strip()
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]
            return json.loads(text)
    except Exception as e:
        logger.warning(f"AIæ¿å—æ˜ å°„å¤±è´¥: {e}")
        return {}


async def enrich_sectors_with_etfs(result: dict):
    """ä¸ºæ¯ä¸ªæ¿å—åŒ¹é…ETFï¼ˆAIæ˜ å°„æ¿å— + æŒ‰æˆäº¤é‡å–Top3ï¼‰"""
    sectors = result.get("sectors", [])
    if not sectors:
        logger.warning("âš ï¸ æ— æ¿å—æ•°æ®ï¼Œè·³è¿‡ETFåŒ¹é…")
        return

    # è¯»å– ETF ä¸»æ•°æ®
    master_file = Path(__file__).parent.parent / "config" / "etf_master.json"
    if not master_file.exists():
        logger.warning("âš ï¸ etf_master.json ä¸å­˜åœ¨")
        return
    etf_master = json.loads(master_file.read_text())
    master_sectors = etf_master.get("sector_list", [])
    sector_index = etf_master.get("sectors", {})
    etfs_data = etf_master.get("etfs", {})
    logger.info(f"ğŸ“Š ETFä¸»æ•°æ®: {len(etfs_data)} ä¸ªETF, {len(master_sectors)} ä¸ªæ¿å—")

    # AI å°†åˆ†ææ¿å—æ˜ å°„åˆ° master æ ‡å‡†æ¿å—
    ai_sector_names = [s["name"] for s in sectors]
    logger.info(f"ğŸ¤– AI æ˜ å°„æ¿å—: {ai_sector_names}")
    sector_mapping = await ai_map_to_master_sectors(ai_sector_names, master_sectors)

    if not sector_mapping:
        logger.warning("âš ï¸ AIæ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŒ¹é…")
        sector_mapping = {name: [name] if name in sector_index else [] for name in ai_sector_names}

    # æ ¹æ®æ˜ å°„æ”¶é›† ETF ä»£ç ï¼ˆåˆå¹¶å¤šä¸ªæ¿å—ï¼‰
    sector_etf_codes: dict[str, list[str]] = {}
    for ai_name, master_names in sector_mapping.items():
        codes = []
        for m_name in master_names:
            if m_name in sector_index:
                codes.extend(sector_index[m_name])
        sector_etf_codes[ai_name] = codes
        if master_names:
            logger.info(f"  {ai_name} â†’ {master_names}")

    # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ ETF ä»£ç 
    codes_to_fetch = set()
    for codes in sector_etf_codes.values():
        codes_to_fetch.update(codes)

    if not codes_to_fetch:
        logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…åˆ°ETFä»£ç ")
        return

    # æ‰¹é‡è·å–ETFå®æ—¶æ•°æ®
    logger.info(f"ğŸ“ˆ è·å– {len(codes_to_fetch)} ä¸ªETFå®æ—¶æ•°æ®")
    fund_data = await fund_service.batch_get_funds(list(codes_to_fetch))

    # ä¸ºæ¯ä¸ªæ¿å—æ·»åŠ ETFä¿¡æ¯
    matched = 0
    for sector in sectors:
        sector_name = sector.get("name", "")
        codes = sector_etf_codes.get(sector_name, [])
        etfs = []
        for code in codes:
            if code in fund_data:
                etfs.append(fund_data[code])
        etfs.sort(key=lambda x: x.get("amount_yi", 0), reverse=True)
        sector["etfs"] = etfs[:3]
        if etfs:
            matched += 1
            logger.info(f"  âœ… {sector_name}: {', '.join(e['name'] for e in etfs[:3])}")

    logger.info(f"âœ… ETFåŒ¹é…å®Œæˆ: {matched}/{len(sectors)} ä¸ªæ¿å—")


if __name__ == "__main__":
    asyncio.run(run())
