"""ç®€åŒ–ç‰ˆæŠ•èµ„åˆ†æ - æ— æ•°æ®åº“ï¼Œå®æ—¶åˆ†æ"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from collections import Counter
from loguru import logger
import httpx

from src.config import settings
from src.models import NewsItem
from src.collectors import NewsAggregator


# å…¨å±€ç¼“å­˜
_cache = {
    "result": None,
    "updated_at": None,
    "news_count": 0,
    "source_stats": {},  # å„æ¥æºé‡‡é›†ç»Ÿè®¡
}

# å®šæ—¶ä»»åŠ¡æ§åˆ¶
_scheduler_task = None

ANALYSIS_PROMPT = """ä½ æ˜¯Aè‚¡ETFæŠ•èµ„åˆ†æå¸ˆï¼Œä¸“æ³¨æ¿å—è½®åŠ¨å’ŒETFé…ç½®å»ºè®®ã€‚

## æ ¸å¿ƒäº¤æ˜“ç†å¿µï¼ˆå¿…é¡»éµå®ˆï¼‰

### 1. æ¿å—è½®åŠ¨è§„å¾‹
- æ”¿ç­–é©±åŠ¨ > ä¸šç»©é©±åŠ¨ > èµ„é‡‘é©±åŠ¨
- ä¸»çº¿æ¿å—æŒç»­æ€§å¼ºï¼Œè·Ÿé£æ¿å—ä¸€æ—¥æ¸¸
- æ¿å—è§é¡¶ä¿¡å·ï¼šé¾™å¤´æ»æ¶¨ã€è¡¥æ¶¨è‚¡æ´»è·ƒ

### 2. ETFé…ç½®åŸåˆ™
- è¶‹åŠ¿ç¡®ç«‹åä»‹å…¥ï¼Œä¸æŠ„åº•ä¸è¿½é«˜
- æ¿å—çƒ­åº¦â‰¥4ä¸”æ–¹å‘åˆ©å¥½æ—¶å¯å…³æ³¨
- è¿ç»­åˆ©ç©ºæˆ–çƒ­åº¦éª¤é™æ—¶å›é¿

### 3. é£é™©è¯†åˆ«è¦ç‚¹
- ğŸš¨ æ”¿ç­–åˆ©ç©ºï¼ˆç›‘ç®¡ã€é™åˆ¶ã€å¤„ç½šï¼‰
- ğŸš¨ è¡Œä¸šæ™¯æ°”ä¸‹è¡Œï¼ˆä¸šç»©é¢„äºã€äº§èƒ½è¿‡å‰©ï¼‰
- ğŸš¨ èµ„é‡‘å‡ºé€ƒï¼ˆåŒ—å‘å¤§å¹…æµå‡ºã€ä¸»åŠ›å‡ä»“ï¼‰

## æ–°é—»æ•°æ®ï¼ˆå…±{count}æ¡ï¼‰
{news_list}

{history_context}

## å¯é€‰æ¿å—
{sector_list}

## å•†å“å‘¨æœŸè§„å¾‹
é»„é‡‘â†’ç™½é“¶â†’é“œâ†’çŸ³æ²¹â†’å†œäº§å“ï¼ˆä¾æ¬¡ä¼ å¯¼ï¼Œé¢†æ¶¨å“ç§åˆ‡æ¢è¡¨ç¤ºå‘¨æœŸæ¼”è¿›ï¼‰

## è¾“å‡ºJSON
```json
{{
  "market_view": "ğŸ¯ ä¸€å¥è¯æ ¸å¿ƒç»“è®ºï¼ˆ25å­—å†…ï¼Œç›´æ¥è¯´ä»Šå¤©è¯¥å…³æ³¨ä»€ä¹ˆï¼‰",
  "summary": "å¸‚åœºç»¼è¿°ï¼ˆ200å­—ï¼‰ï¼šèåˆå…³é”®äº‹å®ä¸è¶‹åŠ¿ï¼Œç”¨emojiæ ‡æ³¨é‡ç‚¹",
  "sentiment": "åä¹è§‚/åæ‚²è§‚/åˆ†æ­§/å¹³æ·¡",
  "sectors": [
    {{
      "name": "æ¿å—åï¼ˆä»å¯é€‰æ¿å—é€‰ï¼‰",
      "heat": 5,
      "direction": "åˆ©å¥½/åˆ©ç©º/ä¸­æ€§",
      "analysis": "æ¿å—åˆ†æï¼ˆ80å­—ï¼‰ï¼šåŒ…å«é©±åŠ¨å› ç´ +é£é™©æç¤º",
      "signal": "ğŸŸ¢ä¹°å…¥/ğŸŸ¡è§‚æœ›/ğŸ”´å›é¿",
      "checklist": ["âœ… æ”¿ç­–æ”¯æŒ", "âœ… ä¸šç»©å‘å¥½", "âš ï¸ ä¼°å€¼åé«˜"]
    }}
  ],
  "risk_alerts": ["é£é™©1ï¼šå…·ä½“æè¿°", "é£é™©2ï¼šå…·ä½“æè¿°"],
  "opportunity_hints": ["æœºä¼š1ï¼šå…·ä½“æè¿°", "æœºä¼š2ï¼šå…·ä½“æè¿°"],
  "commodity_cycle": {{
    "stage": 2,
    "stage_name": "ç™½é“¶è·Ÿæ¶¨æœŸ",
    "leader": "gold/silver/copper/oil/corn",
    "analysis": "å‘¨æœŸåˆ†æï¼ˆ30å­—ï¼‰"
  }}
}}
```

## è¾“å‡ºè¦æ±‚
1. market_view: ä¸€å¥è¯è¯´æ¸…ä»Šæ—¥ä¸»çº¿ï¼Œæœ‰æ“ä½œæŒ‡å¼•æ€§
2. sectors: æœ€å¤š6ä¸ªæ¿å—ï¼ŒæŒ‰çƒ­åº¦æ’åº
   - signal: åŸºäºçƒ­åº¦+æ–¹å‘+é£é™©ç»¼åˆåˆ¤æ–­
   - checklist: 3é¡¹æ£€æŸ¥ï¼Œç”¨âœ…âš ï¸âŒæ ‡è®°
3. risk_alerts: ä»Šæ—¥éœ€è­¦æƒ•çš„2-3ä¸ªé£é™©ç‚¹
4. opportunity_hints: ä»Šæ—¥å€¼å¾—å…³æ³¨çš„2-3ä¸ªæœºä¼š
5. commodity_cycle.leader: å½“å‰é¢†æ¶¨å•†å“ï¼ˆç”¨äºå‰ç«¯é«˜äº®ï¼‰
6. é‡è¦ï¼šJSONå­—ç¬¦ä¸²ä¸­ç¦æ­¢ä½¿ç”¨ä¸­æ–‡å¼•å·""ï¼Œåªç”¨è‹±æ–‡å¼•å·æˆ–ä¸ç”¨å¼•å·
"""


async def collect_news() -> tuple[list[NewsItem], dict]:
    """é‡‡é›†æ‰€æœ‰æºçš„æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, æ¥æºç»Ÿè®¡)"""
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        # ç»Ÿè®¡å„æ¥æºæ•°é‡
        stats = Counter(item.source for item in news.items)
        return news.items, dict(stats)
    finally:
        await agg.close()


async def analyze(items: list[NewsItem], sector_list: list[str] = None, history_context: str = "") -> dict:
    """AIåˆ†ææ–°é—»

    Args:
        items: æ–°é—»åˆ—è¡¨
        sector_list: å¯é€‰æ¿å—åˆ—è¡¨ï¼ˆä» etf_master.json è¯»å–ï¼‰
        history_context: å†å²åˆ†æä¸Šä¸‹æ–‡ï¼ˆç”¨äºè¶‹åŠ¿å¯¹æ¯”ï¼‰
    """
    base_url = settings.claude_base_url.rstrip("/")
    api_key = settings.claude_api_key
    model = settings.claude_model

    news_list = "\n".join([
        f"{i+1}. [{item.source}] {item.title}"
        for i, item in enumerate(items)
    ])

    # é»˜è®¤æ¿å—åˆ—è¡¨ï¼ˆä¸ etf_master.json åŒæ­¥ï¼Œå«å¸¸ç”¨åˆ«åï¼‰
    if not sector_list:
        sector_list = [
            # ç§‘æŠ€
            "èŠ¯ç‰‡", "åŠå¯¼ä½“", "äººå·¥æ™ºèƒ½", "è½¯ä»¶", "é€šä¿¡", "æœºå™¨äºº", "äº’è”ç½‘",
            # æ–°èƒ½æº
            "å…‰ä¼", "æ–°èƒ½æº", "é”‚ç”µæ± ", "æ–°èƒ½æºè½¦",
            # é‡‘è
            "è¯åˆ¸", "é“¶è¡Œ", "åˆ¸å•†",
            # æ¶ˆè´¹
            "ç™½é…’", "æ¶ˆè´¹", "åŒ»è¯", "åˆ›æ–°è¯", "å®¶ç”µ", "æ±½è½¦",
            # å‘¨æœŸ
            "é»„é‡‘", "è´µé‡‘å±", "æœ‰è‰²", "ç…¤ç‚­", "é’¢é“", "çŸ³æ²¹", "åŒ–å·¥",
            # å…¶ä»–
            "å†›å·¥", "å†œä¸š", "æˆ¿åœ°äº§", "ç”µåŠ›", "ç¯ä¿",
            "æ’ç”Ÿç§‘æŠ€", "æ¸¯è‚¡", "æ¸¸æˆ", "ä¼ åª’",
        ]

    sector_str = "/".join(sector_list)
    prompt = ANALYSIS_PROMPT.format(
        count=len(items),
        news_list=news_list,
        history_context=history_context,
        sector_list=sector_str
    )

    try:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{base_url}/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                },
                json={
                    "model": model,
                    "max_tokens": 4096,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"].strip()

        # æå– JSON
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]

        # å°è¯•è§£æï¼Œå¤±è´¥åˆ™ä¿®å¤å¸¸è§é—®é¢˜
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            import re
            logger.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            # ä¿®å¤ï¼šä¸­æ–‡å¼•å·æ›¿æ¢
            text = text.replace("â€œ", '"').replace("â€", '"')
            # ä¿®å¤ï¼šç§»é™¤å°¾éƒ¨é€—å·
            text = re.sub(r',(\s*[}\]])', r'\1', text)
            # ä¿®å¤ï¼šå­—ç¬¦ä¸²å†…çš„æ¢è¡Œï¼ˆæ›´å½»åº•çš„æ–¹æ³•ï¼‰
            def fix_newlines(m):
                return m.group(0).replace('\n', ' ').replace('\r', '')
            text = re.sub(r'"[^"]*"', fix_newlines, text)
            try:
                return json.loads(text)
            except json.JSONDecodeError as e2:
                logger.error(f"ä¿®å¤åä»å¤±è´¥: {e2}")
                logger.error(f"é—®é¢˜æ–‡æœ¬ç‰‡æ®µ: {text[max(0,e2.pos-50):e2.pos+50]}")
                raise
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
        return {}


async def refresh() -> dict:
    """åˆ·æ–°åˆ†æç»“æœ"""
    global _cache

    logger.info("å¼€å§‹é‡‡é›†æ–°é—»...")
    items, source_stats = await collect_news()
    logger.info(f"é‡‡é›†åˆ° {len(items)} æ¡æ–°é—»: {source_stats}")

    logger.info("å¼€å§‹AIåˆ†æ...")
    result = await analyze(items)

    beijing_tz = timezone(timedelta(hours=8))
    _cache = {
        "result": result,
        "updated_at": datetime.now(beijing_tz),
        "news_count": len(items),
        "source_stats": source_stats,
    }

    logger.info("åˆ†æå®Œæˆ")
    return result


def get_cache() -> dict:
    """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    return _cache


async def get_or_refresh(max_age_minutes: int = 60) -> dict:
    """è·å–ç»“æœï¼Œè¿‡æœŸåˆ™åˆ·æ–°"""
    global _cache

    if _cache["result"] is None:
        return await refresh()

    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    age = now - _cache["updated_at"]

    if age.total_seconds() > max_age_minutes * 60:
        return await refresh()

    return _cache["result"]


async def _scheduler_loop(interval_minutes: int = 30):
    """å®šæ—¶åˆ·æ–°å¾ªç¯"""
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            logger.info(f"å®šæ—¶åˆ·æ–°å¼€å§‹ (é—´éš” {interval_minutes} åˆ†é’Ÿ)")
            await refresh()
        except asyncio.CancelledError:
            logger.info("å®šæ—¶ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"å®šæ—¶åˆ·æ–°å¤±è´¥: {e}")


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task is None:
        _scheduler_task = asyncio.create_task(_scheduler_loop(interval_minutes))
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
        logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
