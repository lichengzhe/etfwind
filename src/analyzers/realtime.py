"""ç®€åŒ–ç‰ˆæŠ•èµ„åˆ†æ - æ— æ•°æ®åº“ï¼Œå®æ—¶åˆ†æ"""

import asyncio
from datetime import datetime, timezone, timedelta
from collections import Counter
from loguru import logger
from src.config import settings
from src.models import NewsItem
from src.collectors import NewsAggregator
from src.services.ai_client import AIClient, AIRequest, parse_json_with_repair


# å…¨å±€ç¼“å­˜
_cache = {
    "result": None,
    "updated_at": None,
    "news_count": 0,
    "source_stats": {},  # å„æ¥æºé‡‡é›†ç»Ÿè®¡
}

# å®šæ—¶ä»»åŠ¡æ§åˆ¶
_scheduler_task = None

ANALYSIS_PROMPT = """ä½ æ˜¯Aè‚¡ETFæŠ•èµ„åˆ†æå¸ˆï¼Œä¸“æ³¨æ¿å—è½®åŠ¨å’ŒETFé…ç½®å»ºè®®ã€‚

## æ ¸å¿ƒäº¤æ˜“ç†å¿µï¼ˆå¿…é¡»éµå®ˆï¼‰

### 1. æ¿å—è½®åŠ¨è§„å¾‹
- æ”¿ç­–é©±åŠ¨ > ä¸šç»©é©±åŠ¨ > èµ„é‡‘é©±åŠ¨
- ä¸»çº¿æ¿å—æŒç»­æ€§å¼ºï¼Œè·Ÿé£æ¿å—ä¸€æ—¥æ¸¸
- æ¿å—è§é¡¶ä¿¡å·ï¼šé¾™å¤´æ»æ¶¨ã€è¡¥æ¶¨è‚¡æ´»è·ƒ

### 2. ETFé…ç½®åŸåˆ™
- è¶‹åŠ¿ç¡®ç«‹åä»‹å…¥ï¼Œä¸æŠ„åº•ä¸è¿½é«˜
- æ¿å—çƒ­åº¦â‰¥4ä¸”æ–¹å‘åˆ©å¥½æ—¶å¯å…³æ³¨
- è¿ç»­åˆ©ç©ºæˆ–çƒ­åº¦éª¤é™æ—¶å›é¿

### 3. é£é™©è¯†åˆ«è¦ç‚¹
- ğŸš¨ æ”¿ç­–åˆ©ç©ºï¼ˆç›‘ç®¡ã€é™åˆ¶ã€å¤„ç½šï¼‰
- ğŸš¨ è¡Œä¸šæ™¯æ°”ä¸‹è¡Œï¼ˆä¸šç»©é¢„äºã€äº§èƒ½è¿‡å‰©ï¼‰
- ğŸš¨ èµ„é‡‘å‡ºé€ƒï¼ˆåŒ—å‘å¤§å¹…æµå‡ºã€ä¸»åŠ›å‡ä»“ï¼‰

## æ–°é—»æ•°æ®ï¼ˆå…±{count}æ¡ï¼‰
{news_list}

{history_context}

## å¯é€‰æ¿å—
{sector_list}

## å•†å“å‘¨æœŸè§„å¾‹
é»„é‡‘â†’ç™½é“¶â†’é“œâ†’çŸ³æ²¹â†’å†œäº§å“ï¼ˆä¾æ¬¡ä¼ å¯¼ï¼Œé¢†æ¶¨å“ç§åˆ‡æ¢è¡¨ç¤ºå‘¨æœŸæ¼”è¿›ï¼‰

## è¾“å‡ºJSON
```json
{{
  "market_view": "ğŸ¯ ä¸€å¥è¯æ ¸å¿ƒç»“è®ºï¼ˆ25å­—å†…ï¼Œç›´æ¥è¯´ä»Šå¤©è¯¥å…³æ³¨ä»€ä¹ˆï¼‰",
  "summary": "å¸‚åœºç»¼è¿°ï¼ˆ200å­—ï¼‰ï¼šèåˆå…³é”®äº‹å®ä¸è¶‹åŠ¿ï¼Œç”¨emojiæ ‡æ³¨é‡ç‚¹",
  "sentiment": "åä¹è§‚/åæ‚²è§‚/åˆ†æ­§/å¹³æ·¡",
  "sectors": [
    {{
      "name": "æ¿å—åï¼ˆä»å¯é€‰æ¿å—é€‰ï¼‰",
      "heat": 5,
      "direction": "åˆ©å¥½/åˆ©ç©º/ä¸­æ€§",
      "confidence": 80,
      "analysis": "æ¿å—åˆ†æï¼ˆ80å­—ï¼‰ï¼šåŒ…å«é©±åŠ¨å› ç´ +é£é™©æç¤º",
      "signal": "ğŸŸ¢ä¹°å…¥/ğŸŸ¡è§‚æœ›/ğŸ”´å›é¿"
    }}
  ],
  "risk_alerts": ["é£é™©1ï¼šå…·ä½“æè¿°", "é£é™©2ï¼šå…·ä½“æè¿°"],
  "opportunity_hints": ["æœºä¼š1ï¼šå…·ä½“æè¿°", "æœºä¼š2ï¼šå…·ä½“æè¿°"],
  "commodity_cycle": {{
    "stage": 2,
    "stage_name": "ç™½é“¶è·Ÿæ¶¨æœŸ",
    "leader": "gold/silver/copper/oil/corn",
    "analysis": "å‘¨æœŸåˆ†æï¼ˆ30å­—ï¼‰"
  }}
}}
```

## è¾“å‡ºè¦æ±‚
1. market_view: ä¸€å¥è¯è¯´æ¸…ä»Šæ—¥ä¸»çº¿ï¼Œæœ‰æ“ä½œæŒ‡å¼•æ€§
2. sectors: æœ€å¤š6ä¸ªæ¿å—ï¼ŒæŒ‰çƒ­åº¦æ’åº
   - signal: åŸºäºçƒ­åº¦+æ–¹å‘+é£é™©ç»¼åˆåˆ¤æ–­
   - confidence: 0-100 åˆ†ï¼Œä»£è¡¨ä¿¡å·æŠŠæ¡åº¦
3. risk_alerts: ä»Šæ—¥éœ€è­¦æƒ•çš„2-3ä¸ªé£é™©ç‚¹
4. opportunity_hints: ä»Šæ—¥å€¼å¾—å…³æ³¨çš„2-3ä¸ªæœºä¼š
5. commodity_cycle.leader: å½“å‰é¢†æ¶¨å•†å“ï¼ˆç”¨äºå‰ç«¯é«˜äº®ï¼‰
6. é‡è¦ï¼šJSONå­—ç¬¦ä¸²ä¸­ç¦æ­¢ä½¿ç”¨ä¸­æ–‡å¼•å·""ï¼Œåªç”¨è‹±æ–‡å¼•å·æˆ–ä¸ç”¨å¼•å·
"""


async def collect_news() -> tuple[list[NewsItem], dict]:
    """é‡‡é›†æ‰€æœ‰æºçš„æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, æ¥æºç»Ÿè®¡)"""
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        # ç»Ÿè®¡å„æ¥æºæ•°é‡
        stats = Counter(item.source for item in news.items)
        return news.items, dict(stats)
    finally:
        await agg.close()


async def analyze(items: list[NewsItem], sector_list: list[str] = None, history_context: str = "") -> dict:
    """AIåˆ†ææ–°é—»

    Args:
        items: æ–°é—»åˆ—è¡¨
        sector_list: å¯é€‰æ¿å—åˆ—è¡¨ï¼ˆä» etf_master.json è¯»å–ï¼‰
        history_context: å†å²åˆ†æä¸Šä¸‹æ–‡ï¼ˆç”¨äºè¶‹åŠ¿å¯¹æ¯”ï¼‰
    """
    news_list = "\n".join([
        f"{i+1}. [{item.source}] {item.title}"
        for i, item in enumerate(items)
    ])

    # é»˜è®¤æ¿å—åˆ—è¡¨ï¼ˆä¸ etf_master.json åŒæ­¥ï¼Œå«å¸¸ç”¨åˆ«åï¼‰
    if not sector_list:
        sector_list = [
            # ç§‘æŠ€
            "èŠ¯ç‰‡", "åŠå¯¼ä½“", "äººå·¥æ™ºèƒ½", "è½¯ä»¶", "é€šä¿¡", "æœºå™¨äºº", "äº’è”ç½‘",
            # æ–°èƒ½æº
            "å…‰ä¼", "æ–°èƒ½æº", "é”‚ç”µæ± ", "æ–°èƒ½æºè½¦",
            # é‡‘è
            "è¯åˆ¸", "é“¶è¡Œ", "åˆ¸å•†",
            # æ¶ˆè´¹
            "ç™½é…’", "æ¶ˆè´¹", "åŒ»è¯", "åˆ›æ–°è¯", "å®¶ç”µ", "æ±½è½¦",
            # å‘¨æœŸ
            "é»„é‡‘", "è´µé‡‘å±", "æœ‰è‰²", "ç…¤ç‚­", "é’¢é“", "çŸ³æ²¹", "åŒ–å·¥",
            # å…¶ä»–
            "å†›å·¥", "å†œä¸š", "æˆ¿åœ°äº§", "ç”µåŠ›", "ç¯ä¿",
            "æ’ç”Ÿç§‘æŠ€", "æ¸¯è‚¡", "æ¸¸æˆ", "ä¼ åª’",
        ]

    sector_str = "/".join(sector_list)
    prompt = ANALYSIS_PROMPT.format(
        count=len(items),
        news_list=news_list,
        history_context=history_context,
        sector_list=sector_str
    )

    try:
        client = AIClient()
        text = await client.send(AIRequest(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4096,
            timeout=120,
            model=settings.claude_model,
        ))
        return parse_json_with_repair(text, fix_newlines=True)
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
        return {}


async def refresh() -> dict:
    """åˆ·æ–°åˆ†æç»“æœ"""
    global _cache

    logger.info("å¼€å§‹é‡‡é›†æ–°é—»...")
    items, source_stats = await collect_news()
    logger.info(f"é‡‡é›†åˆ° {len(items)} æ¡æ–°é—»: {source_stats}")

    logger.info("å¼€å§‹AIåˆ†æ...")
    result = await analyze(items)

    beijing_tz = timezone(timedelta(hours=8))
    _cache = {
        "result": result,
        "updated_at": datetime.now(beijing_tz),
        "news_count": len(items),
        "source_stats": source_stats,
    }

    logger.info("åˆ†æå®Œæˆ")
    return result


def get_cache() -> dict:
    """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    return _cache


async def get_or_refresh(max_age_minutes: int = 60) -> dict:
    """è·å–ç»“æœï¼Œè¿‡æœŸåˆ™åˆ·æ–°"""
    global _cache

    if _cache["result"] is None:
        return await refresh()

    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    age = now - _cache["updated_at"]

    if age.total_seconds() > max_age_minutes * 60:
        return await refresh()

    return _cache["result"]


async def _scheduler_loop(interval_minutes: int = 30):
    """å®šæ—¶åˆ·æ–°å¾ªç¯"""
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            logger.info(f"å®šæ—¶åˆ·æ–°å¼€å§‹ (é—´éš” {interval_minutes} åˆ†é’Ÿ)")
            await refresh()
        except asyncio.CancelledError:
            logger.info("å®šæ—¶ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"å®šæ—¶åˆ·æ–°å¤±è´¥: {e}")


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task is None:
        _scheduler_task = asyncio.create_task(_scheduler_loop(interval_minutes))
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
        logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
