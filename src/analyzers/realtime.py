"""ç®€åŒ–ç‰ˆæŠ•èµ„åˆ†æ - æ— æ•°æ®åº“ï¼Œå®æ—¶åˆ†æ"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from collections import Counter
from loguru import logger
import httpx

from src.config import settings
from src.models import NewsItem
from src.collectors import NewsAggregator


# å…¨å±€ç¼“å­˜
_cache = {
    "result": None,
    "updated_at": None,
    "news_count": 0,
    "source_stats": {},  # å„æ¥æºé‡‡é›†ç»Ÿè®¡
}

# å®šæ—¶ä»»åŠ¡æ§åˆ¶
_scheduler_task = None

ANALYSIS_PROMPT = """ä½ æ˜¯Aè‚¡ETFæŠ•èµ„åˆ†æå¸ˆï¼Œåˆ†ææ–°é—»å¹¶è¾“å‡ºæŠ•èµ„å‚è€ƒã€‚

## æ–°é—»ï¼ˆå…±{count}æ¡ï¼‰
{news_list}

{history_context}

## å¯é€‰æ¿å—
{sector_list}

## è¾“å‡ºJSON
```json
{{
  "market_view": "ğŸ¯ å¸‚åœºçŠ¶æ€ä¸€å¥è¯ï¼ˆ20å­—å†…ï¼‰",
  "summary": "ğŸ“Š å¸‚åœºç»¼è¿°ï¼ˆ200-250å­—ï¼‰ï¼šèåˆå…³é”®äº‹å®ä¸è¶‹åŠ¿åˆ†æï¼Œç”¨ğŸ“ˆğŸ“‰ğŸ’°ğŸ”¥ç­‰emojiæ ‡æ³¨é‡ç‚¹æ•°æ®å’Œè½¬æŠ˜ï¼Œå¦‚ã€ŒğŸ“ˆé‡‘ä»·çªç ´5500ç¾å…ƒåˆ›æ–°é«˜ã€ã€ŒğŸ”¥æœºå™¨äººæ¿å—è®¢å•æ”¾é‡ã€ã€‚è¡Œæ–‡æµç•…ï¼Œä¸€æ°”å‘µæˆã€‚",
  "sentiment": "åä¹è§‚",
  "sectors": [
    {{
      "name": "èŠ¯ç‰‡",
      "heat": 5,
      "direction": "åˆ©å¥½",
      "analysis": "æ¿å—åˆ†æï¼ˆ80å­—ï¼‰"
    }}
  ]
}}
```

## è¦æ±‚
- market_view: ä¸€å¥è¯æ¦‚æ‹¬ä»Šæ—¥å¸‚åœºä¸»çº¿
- summary: ç»¼åˆåˆ†æï¼ŒåŒ…å«3-5ä¸ªå…³é”®äº‹å®+è¶‹åŠ¿åˆ¤æ–­ï¼Œç”¨emojiçªå‡ºé‡ç‚¹
- sentiment: æ•´ä½“æƒ…ç»ªï¼ˆåä¹è§‚/åæ‚²è§‚/åˆ†æ­§/å¹³æ·¡ï¼‰
- sectors: æœ€å¤š6ä¸ªï¼ŒæŒ‰çƒ­åº¦æ’åºï¼Œnameå¿…é¡»ä»"å¯é€‰æ¿å—"ä¸­é€‰æ‹©
"""


async def collect_news() -> tuple[list[NewsItem], dict]:
    """é‡‡é›†æ‰€æœ‰æºçš„æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, æ¥æºç»Ÿè®¡)"""
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        # ç»Ÿè®¡å„æ¥æºæ•°é‡
        stats = Counter(item.source for item in news.items)
        return news.items, dict(stats)
    finally:
        await agg.close()


async def analyze(items: list[NewsItem], sector_list: list[str] = None, history_context: str = "") -> dict:
    """AIåˆ†ææ–°é—»

    Args:
        items: æ–°é—»åˆ—è¡¨
        sector_list: å¯é€‰æ¿å—åˆ—è¡¨ï¼ˆä» etf_master.json è¯»å–ï¼‰
        history_context: å†å²åˆ†æä¸Šä¸‹æ–‡ï¼ˆç”¨äºè¶‹åŠ¿å¯¹æ¯”ï¼‰
    """
    base_url = settings.claude_base_url.rstrip("/")
    api_key = settings.claude_api_key
    model = settings.claude_model

    news_list = "\n".join([
        f"{i+1}. [{item.source}] {item.title}"
        for i, item in enumerate(items)
    ])

    # é»˜è®¤æ¿å—åˆ—è¡¨ï¼ˆä¸ etf_master.json åŒæ­¥ï¼Œå«å¸¸ç”¨åˆ«åï¼‰
    if not sector_list:
        sector_list = [
            # ç§‘æŠ€
            "èŠ¯ç‰‡", "åŠå¯¼ä½“", "äººå·¥æ™ºèƒ½", "è½¯ä»¶", "é€šä¿¡", "æœºå™¨äºº", "äº’è”ç½‘",
            # æ–°èƒ½æº
            "å…‰ä¼", "æ–°èƒ½æº", "é”‚ç”µæ± ", "æ–°èƒ½æºè½¦",
            # é‡‘è
            "è¯åˆ¸", "é“¶è¡Œ", "åˆ¸å•†",
            # æ¶ˆè´¹
            "ç™½é…’", "æ¶ˆè´¹", "åŒ»è¯", "åˆ›æ–°è¯", "å®¶ç”µ", "æ±½è½¦",
            # å‘¨æœŸ
            "é»„é‡‘", "è´µé‡‘å±", "æœ‰è‰²", "ç…¤ç‚­", "é’¢é“", "çŸ³æ²¹", "åŒ–å·¥",
            # å…¶ä»–
            "å†›å·¥", "å†œä¸š", "æˆ¿åœ°äº§", "ç”µåŠ›", "ç¯ä¿",
            "æ’ç”Ÿç§‘æŠ€", "æ¸¯è‚¡", "æ¸¸æˆ", "ä¼ åª’",
        ]

    sector_str = "/".join(sector_list)
    prompt = ANALYSIS_PROMPT.format(
        count=len(items),
        news_list=news_list,
        history_context=history_context,
        sector_list=sector_str
    )

    try:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{base_url}/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                },
                json={
                    "model": model,
                    "max_tokens": 4096,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"].strip()

        # æå– JSON
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]

        # å°è¯•è§£æï¼Œå¤±è´¥åˆ™ä¿®å¤å¸¸è§é—®é¢˜
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            import re
            logger.warning(f"JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            # ä¿®å¤ï¼šä¸­æ–‡å¼•å·æ›¿æ¢
            text = text.replace('"', '"').replace('"', '"')
            # ä¿®å¤ï¼šç§»é™¤å°¾éƒ¨é€—å·
            text = re.sub(r',(\s*[}\]])', r'\1', text)
            # ä¿®å¤ï¼šå­—ç¬¦ä¸²å†…çš„æ¢è¡Œï¼ˆæ›´å½»åº•çš„æ–¹æ³•ï¼‰
            def fix_newlines(m):
                return m.group(0).replace('\n', ' ').replace('\r', '')
            text = re.sub(r'"[^"]*"', fix_newlines, text)
            try:
                return json.loads(text)
            except json.JSONDecodeError as e2:
                logger.error(f"ä¿®å¤åä»å¤±è´¥: {e2}")
                logger.error(f"é—®é¢˜æ–‡æœ¬ç‰‡æ®µ: {text[max(0,e2.pos-50):e2.pos+50]}")
                raise
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
        return {}


async def refresh() -> dict:
    """åˆ·æ–°åˆ†æç»“æœ"""
    global _cache

    logger.info("å¼€å§‹é‡‡é›†æ–°é—»...")
    items, source_stats = await collect_news()
    logger.info(f"é‡‡é›†åˆ° {len(items)} æ¡æ–°é—»: {source_stats}")

    logger.info("å¼€å§‹AIåˆ†æ...")
    result = await analyze(items)

    beijing_tz = timezone(timedelta(hours=8))
    _cache = {
        "result": result,
        "updated_at": datetime.now(beijing_tz),
        "news_count": len(items),
        "source_stats": source_stats,
    }

    logger.info("åˆ†æå®Œæˆ")
    return result


def get_cache() -> dict:
    """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    return _cache


async def get_or_refresh(max_age_minutes: int = 60) -> dict:
    """è·å–ç»“æœï¼Œè¿‡æœŸåˆ™åˆ·æ–°"""
    global _cache

    if _cache["result"] is None:
        return await refresh()

    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    age = now - _cache["updated_at"]

    if age.total_seconds() > max_age_minutes * 60:
        return await refresh()

    return _cache["result"]


async def _scheduler_loop(interval_minutes: int = 30):
    """å®šæ—¶åˆ·æ–°å¾ªç¯"""
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            logger.info(f"å®šæ—¶åˆ·æ–°å¼€å§‹ (é—´éš” {interval_minutes} åˆ†é’Ÿ)")
            await refresh()
        except asyncio.CancelledError:
            logger.info("å®šæ—¶ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"å®šæ—¶åˆ·æ–°å¤±è´¥: {e}")


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task is None:
        _scheduler_task = asyncio.create_task(_scheduler_loop(interval_minutes))
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
        logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
