"""ç®€åŒ–ç‰ˆæŠ•èµ„åˆ†æ - æ— æ•°æ®åº“ï¼Œå®æ—¶åˆ†æ"""

import asyncio
import json
from datetime import datetime, timezone, timedelta
from collections import Counter
from loguru import logger
import httpx

from src.config import settings
from src.models import NewsItem
from src.collectors import NewsAggregator


# å…¨å±€ç¼“å­˜
_cache = {
    "result": None,
    "updated_at": None,
    "news_count": 0,
    "source_stats": {},  # å„æ¥æºé‡‡é›†ç»Ÿè®¡
}

# å®šæ—¶ä»»åŠ¡æ§åˆ¶
_scheduler_task = None

ANALYSIS_PROMPT = """ä½ æ˜¯Aè‚¡ETFæŠ•èµ„åˆ†æå¸ˆï¼Œåˆ†ææ–°é—»å¹¶è¾“å‡ºæŠ•èµ„å‚è€ƒã€‚

## æ–°é—»ï¼ˆå…±{count}æ¡ï¼‰
{news_list}

## è¾“å‡ºJSON
```json
{{
  "market_view": "ğŸ¯ å¸‚åœºçŠ¶æ€ä¸€å¥è¯ï¼ˆ20å­—å†…ï¼‰",
  "narrative": "å¸‚åœºå…¨æ™¯åˆ†æï¼ˆ200å­—ï¼Œä¸»è¦çŸ›ç›¾ã€èµ„é‡‘æµå‘ã€æƒ…ç»ªã€è¶‹åŠ¿ï¼Œé€‚å½“emojiï¼‰",
  "sectors": [
    {{
      "name": "èŠ¯ç‰‡",
      "heat": 5,
      "direction": "åˆ©å¥½",
      "analysis": "æ¿å—æ·±åº¦åˆ†æï¼ˆ80-100å­—ï¼ŒåŒ…å«ï¼šé©±åŠ¨å› ç´ ã€äº§ä¸šé“¾å½±å“ã€èµ„é‡‘æ€åº¦ã€çŸ­æœŸå±•æœ›ï¼‰",
      "news": [
        "ğŸ”¥ ç¾å…‰æ¶¨5%åˆ›æ–°é«˜ â†’ å­˜å‚¨èŠ¯ç‰‡æ¶¨ä»·å‘¨æœŸç¡®è®¤ï¼Œå›½äº§æ›¿ä»£åŠ é€Ÿ"
      ]
    }}
  ],
  "risk_level": "ä¸­"
}}
```

## è¦æ±‚
- sectors æœ€å¤š6ä¸ªï¼ŒæŒ‰çƒ­åº¦å’Œé‡è¦æ€§æ’åº
- name å¿…é¡»æ˜¯ï¼šèŠ¯ç‰‡/åŠå¯¼ä½“/äººå·¥æ™ºèƒ½/é€šä¿¡/æœºå™¨äºº/å…‰ä¼/æ–°èƒ½æº/æ–°èƒ½æºè½¦/é”‚ç”µæ± /å†›å·¥/åŒ»è¯/åˆ›æ–°è¯/è¯åˆ¸/é“¶è¡Œ/æˆ¿åœ°äº§/ç™½é…’/æ¶ˆè´¹/å†œä¸š/é»„é‡‘/è´µé‡‘å±/æœ‰è‰²/ç…¤ç‚­/é’¢é“/çŸ³æ²¹/æ’ç”Ÿç§‘æŠ€/æ¸¯è‚¡/æ¸¸æˆ/ä¼ åª’/ç”µåŠ›/äº’è”ç½‘/æ±½è½¦/å®¶ç”µ/ç¯ä¿
- heat: 1-5æ˜Ÿçƒ­åº¦ï¼ˆ5=æçƒ­ï¼ŒåŸºäºæ–°é—»æ•°é‡ã€äº‹ä»¶é‡è¦æ€§ã€å¸‚åœºå…³æ³¨åº¦ï¼‰
- direction: åˆ©å¥½/åˆ©ç©º/ä¸­æ€§
- analysis: æ·±åº¦åˆ†æï¼Œä¿¡æ¯é‡è¦è¶³ï¼ŒåŒ…å«é€»è¾‘é“¾æ¡
- news: 1-3æ¡ï¼Œæ ¼å¼"ğŸ“° æ¶ˆæ¯ â†’ è§£è¯»"ï¼Œæ¶ˆæ¯å’Œè§£è¯»åœ¨ä¸€è¡Œ
- risk_level: ä½/ä¸­/é«˜
"""


async def collect_news() -> tuple[list[NewsItem], dict]:
    """é‡‡é›†æ‰€æœ‰æºçš„æ–°é—»ï¼Œè¿”å› (æ–°é—»åˆ—è¡¨, æ¥æºç»Ÿè®¡)"""
    agg = NewsAggregator(include_international=True, include_playwright=True)
    try:
        news = await agg.collect_all()
        # ç»Ÿè®¡å„æ¥æºæ•°é‡
        stats = Counter(item.source for item in news.items)
        return news.items, dict(stats)
    finally:
        await agg.close()


async def analyze(items: list[NewsItem]) -> dict:
    """AIåˆ†ææ–°é—»"""
    base_url = settings.claude_base_url.rstrip("/")
    api_key = settings.claude_api_key
    model = settings.claude_model

    news_list = "\n".join([
        f"{i+1}. [{item.source}] {item.title}"
        for i, item in enumerate(items)
    ])

    prompt = ANALYSIS_PROMPT.format(count=len(items), news_list=news_list)

    try:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{base_url}/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                },
                json={
                    "model": model,
                    "max_tokens": 4096,
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            resp.raise_for_status()
            data = resp.json()
            text = data["content"][0]["text"].strip()

        # æå– JSON
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]

        return json.loads(text)
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {e}")
        return {}


async def refresh() -> dict:
    """åˆ·æ–°åˆ†æç»“æœ"""
    global _cache

    logger.info("å¼€å§‹é‡‡é›†æ–°é—»...")
    items, source_stats = await collect_news()
    logger.info(f"é‡‡é›†åˆ° {len(items)} æ¡æ–°é—»: {source_stats}")

    logger.info("å¼€å§‹AIåˆ†æ...")
    result = await analyze(items)

    beijing_tz = timezone(timedelta(hours=8))
    _cache = {
        "result": result,
        "updated_at": datetime.now(beijing_tz),
        "news_count": len(items),
        "source_stats": source_stats,
    }

    logger.info("åˆ†æå®Œæˆ")
    return result


def get_cache() -> dict:
    """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    return _cache


async def get_or_refresh(max_age_minutes: int = 60) -> dict:
    """è·å–ç»“æœï¼Œè¿‡æœŸåˆ™åˆ·æ–°"""
    global _cache

    if _cache["result"] is None:
        return await refresh()

    beijing_tz = timezone(timedelta(hours=8))
    now = datetime.now(beijing_tz)
    age = now - _cache["updated_at"]

    if age.total_seconds() > max_age_minutes * 60:
        return await refresh()

    return _cache["result"]


async def _scheduler_loop(interval_minutes: int = 30):
    """å®šæ—¶åˆ·æ–°å¾ªç¯"""
    while True:
        try:
            await asyncio.sleep(interval_minutes * 60)
            logger.info(f"å®šæ—¶åˆ·æ–°å¼€å§‹ (é—´éš” {interval_minutes} åˆ†é’Ÿ)")
            await refresh()
        except asyncio.CancelledError:
            logger.info("å®šæ—¶ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            logger.error(f"å®šæ—¶åˆ·æ–°å¤±è´¥: {e}")


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task is None:
        _scheduler_task = asyncio.create_task(_scheduler_loop(interval_minutes))
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
        logger.info("å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
